{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Training with Time-Series Cross-Validation\n",
    "\n",
    "Train XGBoost models for:\n",
    "1. **Regression**: Next-day return prediction\n",
    "2. **Classification**: Up/down direction prediction\n",
    "\n",
    "**Key Features**:\n",
    "- Time-series train/validation/test split with purge gaps\n",
    "- Point-in-time correct features (no look-ahead bias)\n",
    "- Proper backtesting methodology\n",
    "- Model registry integration\n",
    "\n",
    "**Pipeline**: Feature View → Train/Val/Test Split → XGBoost Training → Evaluation → Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    mean_absolute_error, mean_squared_error, r2_score,\n",
    "    roc_auc_score, accuracy_score, classification_report, confusion_matrix\n",
    ")\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils.hopsworks_helpers import get_feature_store, get_model_registry\n",
    "from utils.time_series_splits import get_train_val_test_split\n",
    "import yaml\n",
    "import joblib\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Load config\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to Hopsworks and Load Feature View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Connect to Hopsworks\nprint(\"Connecting to Hopsworks...\")\nfs = get_feature_store()\nprint(f\"✓ Connected to feature store: {fs.name}\")\n\n# Read directly from combined feature group (has both features AND labels)\n# Note: feature_view.get_batch_data() excludes labels, so we read from the feature group\nprint(\"\\nLoading combined feature group...\")\ncombined_fg = fs.get_feature_group('qqq_combined_features', version=1)\nprint(f\"✓ Feature group loaded: {combined_fg.name} v{combined_fg.version}\")\n\n# Read all data\nprint(\"\\nReading data...\")\ndf = combined_fg.read()\nprint(f\"✓ Data loaded: {df.shape}\")\n\n# Verify we have the required columns\nrequired_cols = ['date', 'target_return', 'target_direction']\nmissing_cols = [col for col in required_cols if col not in df.columns]\nif missing_cols:\n    raise ValueError(f\"Missing required columns: {missing_cols}\")\n\nprint(f\"  Date range: {df['date'].min()} to {df['date'].max()}\")\nprint(f\"  Has targets: target_return={'target_return' in df.columns}, target_direction={'target_direction' in df.columns}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-Series Train/Validation/Test Split with Purge Gap\n",
    "\n",
    "Split data chronologically with purge gaps to prevent look-ahead bias from lagged features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Reset index to get date column (feature view may use date as index)\nif 'date' not in df.columns and df.index.name == 'date':\n    df = df.reset_index()\n\n# Convert date to datetime and remove timezone\ndf['date'] = pd.to_datetime(df['date'])\nif hasattr(df['date'].dtype, 'tz') and df['date'].dtype.tz is not None:\n    df['date'] = df['date'].dt.tz_localize(None)\n\n# Sort by date to ensure chronological order\ndf = df.sort_values('date').reset_index(drop=True)\n\n# Define split dates\n# Use 70% train, 15% validation, 15% test (approximately)\nn_samples = len(df)\ntrain_pct = 0.70\nval_pct = 0.15\n\ntrain_end_idx = int(n_samples * train_pct)\nval_end_idx = int(n_samples * (train_pct + val_pct))\n\ntrain_end_date = df.iloc[train_end_idx]['date']\nval_end_date = df.iloc[val_end_idx]['date']\n\nprint(f\"Total samples: {n_samples}\")\nprint(f\"Split configuration:\")\nprint(f\"  Train: 0 to {train_end_idx} ({train_end_idx} samples, {train_pct*100:.0f}%)\")\nprint(f\"  Validation: {train_end_idx} to {val_end_idx} ({val_end_idx - train_end_idx} samples, {val_pct*100:.0f}%)\")\nprint(f\"  Test: {val_end_idx} to {n_samples} ({n_samples - val_end_idx} samples, {(1-train_pct-val_pct)*100:.0f}%)\")\nprint(f\"\\nSplit dates:\")\nprint(f\"  Train end date: {train_end_date}\")\nprint(f\"  Validation end date: {val_end_date}\")\n\n# Perform split with purge gap of 1 day\npurge_gap = 1\nprint(f\"\\nPerforming time-series split with purge_gap={purge_gap}...\")\n\ntrain_df, val_df, test_df = get_train_val_test_split(\n    df,\n    train_end_date=train_end_date.strftime('%Y-%m-%d'),\n    val_end_date=val_end_date.strftime('%Y-%m-%d'),\n    date_col='date',\n    purge_gap=purge_gap\n)\n\nprint(f\"\\n✓ Split completed:\")\nprint(f\"  Train: {len(train_df)} samples ({train_df['date'].min()} to {train_df['date'].max()})\")\nprint(f\"  Validation: {len(val_df)} samples ({val_df['date'].min()} to {val_df['date'].max()})\")\nprint(f\"  Test: {len(test_df)} samples ({test_df['date'].min()} to {test_df['date'].max()})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Features and Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define feature columns (exclude date, qqq_close, and targets)\nfeature_cols = [col for col in train_df.columns \n                if col not in ['date', 'qqq_close', 'target_return', 'target_direction']]\n\nprint(f\"Feature columns ({len(feature_cols)}):\")\nfor i, col in enumerate(feature_cols, 1):\n    print(f\"  {i:2d}. {col}\")\n\n# Split into features and targets\nX_train = train_df[feature_cols]\ny_train_return = train_df['target_return']\ny_train_direction = train_df['target_direction']\n\nX_val = val_df[feature_cols]\ny_val_return = val_df['target_return']\ny_val_direction = val_df['target_direction']\n\nX_test = test_df[feature_cols]\ny_test_return = test_df['target_return']\ny_test_direction = test_df['target_direction']\n\nprint(f\"\\n✓ Data prepared:\")\nprint(f\"  Train: X={X_train.shape}, y_return={y_train_return.shape}, y_direction={y_train_direction.shape}\")\nprint(f\"  Val:   X={X_val.shape}, y_return={y_val_return.shape}, y_direction={y_val_direction.shape}\")\nprint(f\"  Test:  X={X_test.shape}, y_return={y_test_return.shape}, y_direction={y_test_direction.shape}\")\n\n# Check for missing values\nassert X_train.isnull().sum().sum() == 0, \"Training features have missing values!\"\nassert X_val.isnull().sum().sum() == 0, \"Validation features have missing values!\"\nassert X_test.isnull().sum().sum() == 0, \"Test features have missing values!\"\nprint(\"\\n✓ No missing values in features\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Regression Model (Next-Day Return Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize XGBoost regressor\nprint(\"Training XGBoost regression model...\")\nxgb_regressor = xgb.XGBRegressor(\n    n_estimators=config['model']['xgboost']['regression']['n_estimators'],\n    max_depth=config['model']['xgboost']['regression']['max_depth'],\n    learning_rate=config['model']['xgboost']['regression']['learning_rate'],\n    random_state=config['model']['xgboost']['regression']['random_state'],\n    objective='reg:squarederror',\n    tree_method='hist',  # Faster training\n    eval_metric='rmse'  # Moved here from fit()\n)\n\n# Train with validation set monitoring\nxgb_regressor.fit(\n    X_train, y_train_return,\n    eval_set=[(X_train, y_train_return), (X_val, y_val_return)],\n    verbose=20\n)\n\nprint(\"\\n✓ Regression model trained!\")\n\n# Get predictions on all sets\ny_train_pred_return = xgb_regressor.predict(X_train)\ny_val_pred_return = xgb_regressor.predict(X_val)\ny_test_pred_return = xgb_regressor.predict(X_test)\n\nprint(f\"\\nPredictions generated:\")\nprint(f\"  Train: {len(y_train_pred_return)} predictions\")\nprint(f\"  Validation: {len(y_val_pred_return)} predictions\")\nprint(f\"  Test: {len(y_test_pred_return)} predictions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all sets\n",
    "def calculate_regression_metrics(y_true, y_pred, set_name):\n",
    "    \"\"\"Calculate and display regression metrics\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Directional accuracy (did we predict the correct sign?)\n",
    "    directional_acc = accuracy_score(\n",
    "        (y_true > 0).astype(int),\n",
    "        (y_pred > 0).astype(int)\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics:\")\n",
    "    print(f\"  MAE:  {mae:.6f}\")\n",
    "    print(f\"  RMSE: {rmse:.6f}\")\n",
    "    print(f\"  R²:   {r2:.6f}\")\n",
    "    print(f\"  Directional Accuracy: {directional_acc:.4f} ({directional_acc*100:.2f}%)\")\n",
    "    \n",
    "    return {'mae': mae, 'rmse': rmse, 'r2': r2, 'directional_accuracy': directional_acc}\n",
    "\n",
    "# Evaluate on all sets\n",
    "print(\"=\"*60)\n",
    "print(\"REGRESSION MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_metrics = calculate_regression_metrics(y_train_return, y_train_pred_return, \"Training\")\n",
    "val_metrics = calculate_regression_metrics(y_val_return, y_val_pred_return, \"Validation\")\n",
    "test_metrics = calculate_regression_metrics(y_test_return, y_test_pred_return, \"Test\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"MAE:  {test_metrics['mae']:.6f}\")\n",
    "print(f\"RMSE: {test_metrics['rmse']:.6f}\")\n",
    "print(f\"R²:   {test_metrics['r2']:.6f}\")\n",
    "print(f\"Directional Accuracy: {test_metrics['directional_accuracy']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Regression Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions vs actuals\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Test set: Predicted vs Actual scatter\n",
    "ax = axes[0, 0]\n",
    "ax.scatter(y_test_return, y_test_pred_return, alpha=0.5, s=30)\n",
    "ax.plot([y_test_return.min(), y_test_return.max()], \n",
    "        [y_test_return.min(), y_test_return.max()], 'r--', lw=2)\n",
    "ax.set_xlabel('Actual Return')\n",
    "ax.set_ylabel('Predicted Return')\n",
    "ax.set_title('Test Set: Predicted vs Actual Returns')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.text(0.05, 0.95, f'R² = {test_metrics[\"r2\"]:.4f}\\nRMSE = {test_metrics[\"rmse\"]:.6f}', \n",
    "        transform=ax.transAxes, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Test set: Time series plot\n",
    "ax = axes[0, 1]\n",
    "test_dates = test_df['date'].values\n",
    "ax.plot(test_dates, y_test_return.values, label='Actual', alpha=0.7, linewidth=2)\n",
    "ax.plot(test_dates, y_test_pred_return, label='Predicted', alpha=0.7, linewidth=2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Return')\n",
    "ax.set_title('Test Set: Time Series of Returns')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Residuals distribution\n",
    "ax = axes[1, 0]\n",
    "residuals = y_test_return - y_test_pred_return\n",
    "ax.hist(residuals, bins=30, edgecolor='black', alpha=0.7)\n",
    "ax.axvline(0, color='r', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Prediction Error (Actual - Predicted)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title(f'Residuals Distribution (Mean={residuals.mean():.6f})')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals over time\n",
    "ax = axes[1, 1]\n",
    "ax.scatter(test_dates, residuals, alpha=0.5, s=30)\n",
    "ax.axhline(0, color='r', linestyle='--', linewidth=2)\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Prediction Error')\n",
    "ax.set_title('Residuals Over Time')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Regression visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Classification Model (Up/Down Direction Prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize XGBoost classifier\nprint(\"Training XGBoost classification model...\")\nxgb_classifier = xgb.XGBClassifier(\n    n_estimators=config['model']['xgboost']['classification']['n_estimators'],\n    max_depth=config['model']['xgboost']['classification']['max_depth'],\n    learning_rate=config['model']['xgboost']['classification']['learning_rate'],\n    random_state=config['model']['xgboost']['classification']['random_state'],\n    objective='binary:logistic',\n    tree_method='hist',  # Faster training\n    eval_metric='logloss'  # Moved here from fit()\n)\n\n# Train with validation set monitoring\nxgb_classifier.fit(\n    X_train, y_train_direction,\n    eval_set=[(X_train, y_train_direction), (X_val, y_val_direction)],\n    verbose=20\n)\n\nprint(\"\\n✓ Classification model trained!\")\n\n# Get predictions on all sets\ny_train_pred_direction = xgb_classifier.predict(X_train)\ny_train_pred_proba = xgb_classifier.predict_proba(X_train)[:, 1]\n\ny_val_pred_direction = xgb_classifier.predict(X_val)\ny_val_pred_proba = xgb_classifier.predict_proba(X_val)[:, 1]\n\ny_test_pred_direction = xgb_classifier.predict(X_test)\ny_test_pred_proba = xgb_classifier.predict_proba(X_test)[:, 1]\n\nprint(f\"\\nPredictions generated:\")\nprint(f\"  Train: {len(y_train_pred_direction)} predictions\")\nprint(f\"  Validation: {len(y_val_pred_direction)} predictions\")\nprint(f\"  Test: {len(y_test_pred_direction)} predictions\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for all sets\n",
    "def calculate_classification_metrics(y_true, y_pred, y_proba, set_name):\n",
    "    \"\"\"Calculate and display classification metrics\"\"\"\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    auc = roc_auc_score(y_true, y_proba)\n",
    "    \n",
    "    print(f\"\\n{set_name} Metrics:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  AUC-ROC:  {auc:.4f}\")\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    print(f\"\\n  Confusion Matrix:\")\n",
    "    print(f\"                Predicted\")\n",
    "    print(f\"                Down  Up\")\n",
    "    print(f\"  Actual Down   {cm[0,0]:4d}  {cm[0,1]:4d}\")\n",
    "    print(f\"         Up     {cm[1,0]:4d}  {cm[1,1]:4d}\")\n",
    "    \n",
    "    return {'accuracy': accuracy, 'auc': auc, 'confusion_matrix': cm}\n",
    "\n",
    "# Evaluate on all sets\n",
    "print(\"=\"*60)\n",
    "print(\"CLASSIFICATION MODEL EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "train_class_metrics = calculate_classification_metrics(\n",
    "    y_train_direction, y_train_pred_direction, y_train_pred_proba, \"Training\"\n",
    ")\n",
    "val_class_metrics = calculate_classification_metrics(\n",
    "    y_val_direction, y_val_pred_direction, y_val_pred_proba, \"Validation\"\n",
    ")\n",
    "test_class_metrics = calculate_classification_metrics(\n",
    "    y_test_direction, y_test_pred_direction, y_test_pred_proba, \"Test\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Accuracy: {test_class_metrics['accuracy']:.4f}\")\n",
    "print(f\"AUC-ROC:  {test_class_metrics['auc']:.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    y_test_direction, y_test_pred_direction, \n",
    "    target_names=['Down (0)', 'Up (1)'],\n",
    "    digits=4\n",
    "))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Classification Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize classification results\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# ROC Curve\n",
    "ax = axes[0, 0]\n",
    "fpr, tpr, _ = roc_curve(y_test_direction, y_test_pred_proba)\n",
    "ax.plot(fpr, tpr, linewidth=2, label=f'ROC (AUC = {test_class_metrics[\"auc\"]:.4f})')\n",
    "ax.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('ROC Curve (Test Set)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Confusion Matrix Heatmap\n",
    "ax = axes[0, 1]\n",
    "cm = test_class_metrics['confusion_matrix']\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax,\n",
    "            xticklabels=['Down', 'Up'], yticklabels=['Down', 'Up'])\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('Actual')\n",
    "ax.set_title('Confusion Matrix (Test Set)')\n",
    "\n",
    "# Prediction probabilities distribution\n",
    "ax = axes[1, 0]\n",
    "ax.hist(y_test_pred_proba[y_test_direction == 0], bins=30, alpha=0.5, label='Actual Down', edgecolor='black')\n",
    "ax.hist(y_test_pred_proba[y_test_direction == 1], bins=30, alpha=0.5, label='Actual Up', edgecolor='black')\n",
    "ax.axvline(0.5, color='r', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax.set_xlabel('Predicted Probability (Up)')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Predicted Probabilities Distribution')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Predictions over time\n",
    "ax = axes[1, 1]\n",
    "test_dates = test_df['date'].values\n",
    "colors = ['red' if actual == pred else 'green' for actual, pred in zip(y_test_direction, y_test_pred_direction)]\n",
    "ax.scatter(test_dates, y_test_pred_proba, c=colors, alpha=0.6, s=50)\n",
    "ax.axhline(0.5, color='black', linestyle='--', linewidth=2, label='Decision Threshold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Predicted Probability (Up)')\n",
    "ax.set_title('Predictions Over Time (Red=Correct, Green=Incorrect)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Classification visualizations complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "regressor_path = os.path.join(models_dir, 'qqq_regressor.pkl')\n",
    "classifier_path = os.path.join(models_dir, 'qqq_classifier.pkl')\n",
    "\n",
    "joblib.dump(xgb_regressor, regressor_path)\n",
    "joblib.dump(xgb_classifier, classifier_path)\n",
    "\n",
    "print(f\"✓ Models saved locally:\")\n",
    "print(f\"  Regressor: {regressor_path}\")\n",
    "print(f\"  Classifier: {classifier_path}\")\n",
    "\n",
    "# Also save model metadata\n",
    "metadata = {\n",
    "    'trained_at': datetime.now().isoformat(),\n",
    "    'feature_count': len(feature_cols),\n",
    "    'training_samples': len(train_df),\n",
    "    'validation_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'regression_metrics': {\n",
    "        'test_mae': float(test_metrics['mae']),\n",
    "        'test_rmse': float(test_metrics['rmse']),\n",
    "        'test_r2': float(test_metrics['r2']),\n",
    "        'test_directional_accuracy': float(test_metrics['directional_accuracy'])\n",
    "    },\n",
    "    'classification_metrics': {\n",
    "        'test_accuracy': float(test_class_metrics['accuracy']),\n",
    "        'test_auc': float(test_class_metrics['auc'])\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(\"\\n✓ Local save complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models Locally (Must run before Model Registry upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Regression model feature importance\n",
    "ax = axes[0]\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_regressor.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_n = 15\n",
    "sns.barplot(\n",
    "    data=feature_importance_reg.head(top_n), \n",
    "    y='feature', \n",
    "    x='importance',\n",
    "    ax=ax,\n",
    "    palette='viridis'\n",
    ")\n",
    "ax.set_title(f'Top {top_n} Features - Regression Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Classification model feature importance\n",
    "ax = axes[1]\n",
    "feature_importance_cls = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "sns.barplot(\n",
    "    data=feature_importance_cls.head(top_n), \n",
    "    y='feature', \n",
    "    x='importance',\n",
    "    ax=ax,\n",
    "    palette='plasma'\n",
    ")\n",
    "ax.set_title(f'Top {top_n} Features - Classification Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRegression Model:\")\n",
    "for i, row in feature_importance_reg.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:25s} : {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Model:\")\n",
    "for i, row in feature_importance_cls.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:25s} : {row['importance']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**✅ Model Training Complete**\n",
    "\n",
    "### Models Trained:\n",
    "1. **XGBoost Regressor** - Predicts next-day QQQ return\n",
    "2. **XGBoost Classifier** - Predicts up/down direction\n",
    "\n",
    "### Key Features:\n",
    "- **Time-Series Split**: 70% train, 15% validation, 15% test with purge gaps\n",
    "- **Point-in-Time Correct**: No look-ahead bias in features\n",
    "- **Feature Count**: 27 engineered features\n",
    "- **Training Method**: XGBoost with validation set monitoring\n",
    "\n",
    "### Model Performance:\n",
    "**Regression Model:**\n",
    "- Test MAE: Check output above\n",
    "- Test RMSE: Check output above  \n",
    "- Test R²: Check output above\n",
    "- Directional Accuracy: Check output above\n",
    "\n",
    "**Classification Model:**\n",
    "- Test Accuracy: Check output above\n",
    "- Test AUC-ROC: Check output above\n",
    "\n",
    "### Saved Artifacts:\n",
    "- Local models: `../models/qqq_regressor.pkl`, `../models/qqq_classifier.pkl`\n",
    "- Model metadata: `../models/model_metadata.json`\n",
    "- Hopsworks registry: Both models registered (if successful)\n",
    "\n",
    "### Next Steps:\n",
    "- **Notebook 8**: Implement daily inference pipeline\n",
    "- Use saved models to generate daily predictions\n",
    "- Create Gradio dashboard for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to model registry\n",
    "print(\"Connecting to Hopsworks Model Registry...\")\n",
    "mr = get_model_registry()\n",
    "print(f\"✓ Connected to model registry\")\n",
    "\n",
    "# Register regression model\n",
    "print(\"\\nRegistering regression model...\")\n",
    "try:\n",
    "    reg_model = mr.python.create_model(\n",
    "        name=\"qqq_return_regressor\",\n",
    "        description=\"XGBoost regression model for predicting next-day QQQ return. Trained with time-series split and point-in-time correct features.\",\n",
    "        metrics={\n",
    "            \"test_mae\": float(test_metrics['mae']),\n",
    "            \"test_rmse\": float(test_metrics['rmse']),\n",
    "            \"test_r2\": float(test_metrics['r2']),\n",
    "            \"test_directional_accuracy\": float(test_metrics['directional_accuracy']),\n",
    "            \"val_mae\": float(val_metrics['mae']),\n",
    "            \"val_rmse\": float(val_metrics['rmse']),\n",
    "        },\n",
    "        input_example=X_train.head(1),\n",
    "        model_schema={\n",
    "            \"input_schema\": X_train.dtypes.to_dict(),\n",
    "            \"output_schema\": {\"predicted_return\": \"float64\"}\n",
    "        }\n",
    "    )\n",
    "    reg_model.save(regressor_path)\n",
    "    print(f\"✓ Regression model registered: {reg_model.name} v{reg_model.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not register regression model: {e}\")\n",
    "    print(\"   Model saved locally but not in registry\")\n",
    "\n",
    "# Register classification model\n",
    "print(\"\\nRegistering classification model...\")\n",
    "try:\n",
    "    cls_model = mr.python.create_model(\n",
    "        name=\"qqq_direction_classifier\",\n",
    "        description=\"XGBoost classification model for predicting QQQ up/down direction. Trained with time-series split and point-in-time correct features.\",\n",
    "        metrics={\n",
    "            \"test_accuracy\": float(test_class_metrics['accuracy']),\n",
    "            \"test_auc\": float(test_class_metrics['auc']),\n",
    "            \"val_accuracy\": float(val_class_metrics['accuracy']),\n",
    "            \"val_auc\": float(val_class_metrics['auc']),\n",
    "        },\n",
    "        input_example=X_train.head(1),\n",
    "        model_schema={\n",
    "            \"input_schema\": X_train.dtypes.to_dict(),\n",
    "            \"output_schema\": {\n",
    "                \"predicted_direction\": \"int64\",\n",
    "                \"predicted_probability\": \"float64\"\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    cls_model.save(classifier_path)\n",
    "    print(f\"✓ Classification model registered: {cls_model.name} v{cls_model.version}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  Could not register classification model: {e}\")\n",
    "    print(\"   Model saved locally but not in registry\")\n",
    "\n",
    "print(\"\\n✓ Model registration complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Models to Hopsworks Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = '../models'\n",
    "os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "regressor_path = os.path.join(models_dir, 'qqq_regressor.pkl')\n",
    "classifier_path = os.path.join(models_dir, 'qqq_classifier.pkl')\n",
    "\n",
    "joblib.dump(xgb_regressor, regressor_path)\n",
    "joblib.dump(xgb_classifier, classifier_path)\n",
    "\n",
    "print(f\"✓ Models saved locally:\")\n",
    "print(f\"  Regressor: {regressor_path}\")\n",
    "print(f\"  Classifier: {classifier_path}\")\n",
    "\n",
    "# Also save model metadata\n",
    "metadata = {\n",
    "    'trained_at': datetime.now().isoformat(),\n",
    "    'feature_count': len(feature_cols),\n",
    "    'training_samples': len(train_df),\n",
    "    'validation_samples': len(val_df),\n",
    "    'test_samples': len(test_df),\n",
    "    'regression_metrics': {\n",
    "        'test_mae': float(test_metrics['mae']),\n",
    "        'test_rmse': float(test_metrics['rmse']),\n",
    "        'test_r2': float(test_metrics['r2']),\n",
    "        'test_directional_accuracy': float(test_metrics['directional_accuracy'])\n",
    "    },\n",
    "    'classification_metrics': {\n",
    "        'test_accuracy': float(test_class_metrics['accuracy']),\n",
    "        'test_auc': float(test_class_metrics['auc'])\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = os.path.join(models_dir, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(\"\\n✓ Local save complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance from both models\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Regression model feature importance\n",
    "ax = axes[0]\n",
    "feature_importance_reg = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_regressor.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "top_n = 15\n",
    "sns.barplot(\n",
    "    data=feature_importance_reg.head(top_n), \n",
    "    y='feature', \n",
    "    x='importance',\n",
    "    ax=ax,\n",
    "    palette='viridis'\n",
    ")\n",
    "ax.set_title(f'Top {top_n} Features - Regression Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Classification model feature importance\n",
    "ax = axes[1]\n",
    "feature_importance_cls = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': xgb_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "sns.barplot(\n",
    "    data=feature_importance_cls.head(top_n), \n",
    "    y='feature', \n",
    "    x='importance',\n",
    "    ax=ax,\n",
    "    palette='plasma'\n",
    ")\n",
    "ax.set_title(f'Top {top_n} Features - Classification Model', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_ylabel('Feature')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TOP 10 MOST IMPORTANT FEATURES\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nRegression Model:\")\n",
    "for i, row in feature_importance_reg.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:25s} : {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Model:\")\n",
    "for i, row in feature_importance_cls.head(10).iterrows():\n",
    "    print(f\"  {row['feature']:25s} : {row['importance']:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.13 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ced7873b16255b563b5eed9541f98d8345d177e40ec25a0630ef59f6ff5e773"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}