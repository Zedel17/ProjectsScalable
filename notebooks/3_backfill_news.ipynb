{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Backfill News Data with FinBERT Sentiment\n",
    "Fetch historical news articles and apply FinBERT sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:25:29.517867Z",
     "start_time": "2026-01-01T17:25:26.454458Z"
    }
   },
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from utils.data_fetchers import fetch_news_articles, apply_finbert_sentiment\n",
    "from utils.hopsworks_helpers import get_feature_store, create_feature_group\n",
    "from dotenv import load_dotenv\n",
    "import yaml\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Load config\n",
    "with open('../config/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## NEWSAPI"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load FinBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T16:08:45.685626Z",
     "start_time": "2026-01-01T16:07:26.592746Z"
    }
   },
   "source": [
    "# Load FinBERT for financial sentiment analysis\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(\"FinBERT model loaded successfully\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/252 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d53ace679d344c19916c105dd4ddf966"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/758 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "aede7fda5342458f8858d8260c4be042"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75ce5572a7c14c9193006da4cf2ffd44"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bd0c55682ab34a059c77a2da638dbb18"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bc0a42d786aa462f881dee5e3c726a54"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT model loaded successfully\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "31ec2ecf03d9485c85336dee034a8f2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch News Articles and Apply Sentiment Analysis\n",
    "\n",
    "**Note:** NewsAPI free tier has limits (100 requests/day). For full backfill, you may need to:\n",
    "- Run this over multiple days\n",
    "- Use a paid plan\n",
    "- Sample specific dates\n",
    "\n",
    "For now, we'll fetch recent news as an example."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:00:34.143483Z",
     "start_time": "2026-01-01T17:00:34.140785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getenv(\"NEWS_API_KEY\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "037d0616-3b8d-4af2-a4ac-5aac1790c074\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T16:43:12.954463Z",
     "start_time": "2026-01-01T16:42:37.297618Z"
    }
   },
   "source": [
    "# For demonstration, fetch last 30 days of news\n",
    "# Adjust this based on your NewsAPI plan\n",
    "import time\n",
    "\n",
    "end_date = datetime.strptime(config['data']['end_date'], '%Y-%m-%d')\n",
    "start_date = end_date - timedelta(days=29)  # Last 30 days for demo\n",
    "\n",
    "query = config['data']['news']['query']\n",
    "all_articles = []\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching news for {date_str}...\")\n",
    "    \n",
    "    try:\n",
    "        articles = fetch_news_articles(query, date_str, max_articles=100)\n",
    "        \n",
    "        for article in articles:\n",
    "            # Combine title and description for sentiment analysis\n",
    "            text = f\"{article.get('title', '')} {article.get('description', '')}\"\n",
    "            \n",
    "            # Apply FinBERT\n",
    "            sentiment = apply_finbert_sentiment(text, model, tokenizer)\n",
    "            \n",
    "            all_articles.append({\n",
    "                'date': date_str,\n",
    "                'title': article.get('title'),\n",
    "                'description': article.get('description'),\n",
    "                'source': article.get('source', {}).get('name'),\n",
    "                'url': article.get('url'),\n",
    "                **sentiment\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {date_str}: {e}\")\n",
    "\n",
    "    # Pausa 1 secondo per evitare rate limit NewsAPI\n",
    "    time.sleep(1)\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "news_df = pd.DataFrame(all_articles)\n",
    "print(f\"\\nTotal articles fetched: {len(news_df)}\")\n",
    "news_df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 2025-12-01...\n",
      "Error fetching news for 2025-12-01: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-02...\n",
      "Error fetching news for 2025-12-02: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-03...\n",
      "Error fetching news for 2025-12-03: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-04...\n",
      "Error fetching news for 2025-12-04: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-05...\n",
      "Error fetching news for 2025-12-05: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-06...\n",
      "Error fetching news for 2025-12-06: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-07...\n",
      "Error fetching news for 2025-12-07: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-08...\n",
      "Error fetching news for 2025-12-08: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-09...\n",
      "Error fetching news for 2025-12-09: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-10...\n",
      "Error fetching news for 2025-12-10: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-11...\n",
      "Error fetching news for 2025-12-11: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-12...\n",
      "Error fetching news for 2025-12-12: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-13...\n",
      "Error fetching news for 2025-12-13: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-14...\n",
      "Error fetching news for 2025-12-14: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-15...\n",
      "Error fetching news for 2025-12-15: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-16...\n",
      "Error fetching news for 2025-12-16: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-17...\n",
      "Error fetching news for 2025-12-17: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-18...\n",
      "Error fetching news for 2025-12-18: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-19...\n",
      "Error fetching news for 2025-12-19: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-20...\n",
      "Error fetching news for 2025-12-20: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-21...\n",
      "Error fetching news for 2025-12-21: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-22...\n",
      "Error fetching news for 2025-12-22: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-23...\n",
      "Error fetching news for 2025-12-23: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-24...\n",
      "Error fetching news for 2025-12-24: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-25...\n",
      "Error fetching news for 2025-12-25: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-26...\n",
      "Error fetching news for 2025-12-26: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-27...\n",
      "Error fetching news for 2025-12-27: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-28...\n",
      "Error fetching news for 2025-12-28: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-29...\n",
      "Error fetching news for 2025-12-29: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "Fetching news for 2025-12-30...\n",
      "Error fetching news for 2025-12-30: {'status': 'error', 'code': 'apiKeyInvalid', 'message': 'Your API key is invalid or incorrect. Check your key, or go to https://newsapi.org to create a free API key.'}\n",
      "\n",
      "Total articles fetched: 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload to Hopsworks Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Hopsworks\n",
    "fs = get_feature_store()\n",
    "\n",
    "# Create feature group for article-level sentiment\n",
    "news_fg = create_feature_group(\n",
    "    fs,\n",
    "    name='news_sentiment_raw',\n",
    "    df=news_df,\n",
    "    primary_key=['date', 'url'],\n",
    "    description='Article-level news sentiment from NewsAPI + FinBERT'\n",
    ")\n",
    "\n",
    "print(\"News sentiment data uploaded to Hopsworks!\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## FINNHUB"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:25:52.570510Z",
     "start_time": "2026-01-01T17:25:52.567896Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "print(os.getenv(\"FINNHUB_API_KEY\"))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d5baqh9r01qj66bgg1t0d5baqh9r01qj66bgg1tg\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:26:01.029513Z",
     "start_time": "2026-01-01T17:26:00.060578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import finnhub\n",
    "import os\n",
    "\n",
    "# Inizializza client\n",
    "finnhub_client = finnhub.Client(api_key=os.getenv(\"FINNHUB_API_KEY\"))\n",
    "\n",
    "# Notizie generali sul mercato\n",
    "general_news = finnhub_client.general_news('forex', min_id=0)\n",
    "print(general_news[:3])\n",
    "\n",
    "# Notizie specifiche su un ticker (ad esempio QQQ)\n",
    "company_news = finnhub_client.company_news('QQQ', _from='2025-12-01', to='2026-01-01')\n",
    "print(company_news[:3])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'category': 'forex', 'datetime': 1767187823, 'headline': 'US initial jobless claims 199K vs 220K expected', 'id': 7565419, 'image': 'https://images.investinglive.com/images/Texas%20jobless%20claims_id_0234a7dc-7fa0-4534-90b8-46797ad79516_size975.jpeg', 'related': '', 'source': 'Forexlive', 'summary': '<ul><li>Prior was 214K (revised to 215K)</li><li>Continuing claims 1.866M vs 1.923M prior</li></ul><p>The claims numbers over the holidays are highly volatile and subject to large seasonal revisions so they\\'re poor numbers to index from.</p><p>The drop over a number of weeks is notable though and is tracking towards the bottom end of this range again. Next week\\'s data will also be highly-subject to holiday seasonality but in early January, watch the numbers.</p><p>The US government shutdown made this a tough report to read through but it\\'s tough to see where the Federal Reserve is seeing weakening in the US jobs market based on this chart. Some policymakers argue it\\'s a \\'low higher, low firing\\' economy and that there is some evidence for that but the \\'low firing part\\' seems to be the most definitive, as shown in claims.</p><p>For some background, weekly initial jobless claims are released every Thursday at 8:30 am ET by the Department of Labor. They track how many Americans filed for unemployment benefits for the first time. Bill Gross said that if he only had one economic indicator, this would be it as it\\'s the ultimate \"high-frequency\" pulse check on the US economy. While the monthly Non-Farm Payrolls gets the glory, jobless claims provide a real-time leading indicator.</p><p>That said, there is a high \\'noise to signal\\' ratio in the report as holidays and other special factors can cause large weekly distortions. That\\'s why many market watchers prefer to look at four-week moving averages in the report. However when you do that, you tend to end up with the same lags as non-farm payrolls. </p><p>So overall, this report is one piece of the puzzle and one that should be watched carefully but taken with a grain of salt, especially around holidays.</p>\\n                            This article was written by Adam Button at investinglive.com.', 'url': 'https://investinglive.com/news/us-initial-jobless-claims-199k-vs-220k-expected-20251231/'}]\n",
      "[{'category': 'company', 'datetime': 1767259836, 'headline': 'SCHG: How This ETF Looks Heading Into 2026', 'id': 137957547, 'image': 'https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1455774438/image_1455774438.jpg?io=getty-c-w1536', 'related': 'QQQ', 'source': 'SeekingAlpha', 'summary': 'SCHG ETF review: low-cost large-cap growth with quality tilt, but high P/E risks. See why SCHG earns a solid \"hold\" and nearly a \"buy\" rating heading into 2026.', 'url': 'https://finnhub.io/api/news?id=1a2ff82de0660bce5882865d7c49655f6ada70da36107006a3a299bee2ea3fea'}, {'category': 'company', 'datetime': 1767254700, 'headline': 'ETF Share Class Approval For Mutual Fund Complexes - A True Game-Changer', 'id': 137957083, 'image': 'https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1161118927/image_1161118927.jpg?io=getty-c-w1536', 'related': 'QQQ', 'source': 'SeekingAlpha', 'summary': 'At the end of November 2025, the SEC formally approved the first applications for mutual fund complexes to offer an ETF share class within a single fund structure. Read more here...', 'url': 'https://finnhub.io/api/news?id=caed299d8c2b852c2bf29c770801c71b1a8d95bfb985196b5aff20b17d3ddecf'}, {'category': 'company', 'datetime': 1767252600, 'headline': \"The Fed's Biggest Problem Is The Market's Greatest Advantage\", 'id': 137956776, 'image': 'https://static.seekingalpha.com/cdn/s3/uploads/getty_images/1070981362/image_1070981362.jpg?io=getty-c-w1536', 'related': 'QQQ', 'source': 'SeekingAlpha', 'summary': 'Passive S&P 500 ETF investors beat most active managers in 2025 as Big Tech drove a K-shaped economy. See more analysis here.', 'url': 'https://finnhub.io/api/news?id=31330928ba8c5aae1bbfeaf34c4c68323443a5c6535996a5f9e7e8c5330f6482'}]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:28:44.707194Z",
     "start_time": "2026-01-01T17:28:43.524722Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load FinBERT for financial sentiment analysis\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "print(\"FinBERT model loaded successfully\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinBERT model loaded successfully\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:35:25.088670Z",
     "start_time": "2026-01-01T17:34:40.855115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#FILTRO ARTICOLI\n",
    "# Fetch last 30 days of news from Finnhub (filtered for QQQ/big tech/macro)\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import finnhub  # pip install finnhub-python\n",
    "\n",
    "# Configura client Finnhub\n",
    "finnhub_api_key = os.getenv(\"FINNHUB_API_KEY\")\n",
    "finnhub_client = finnhub.Client(api_key=finnhub_api_key)\n",
    "\n",
    "# Intervallo temporale: ultimi 30 giorni\n",
    "end_date = datetime.strptime(config['data']['end_date'], '%Y-%m-%d')\n",
    "start_date = end_date - timedelta(days=29)\n",
    "\n",
    "query_keywords = [\n",
    "    \"QQQ\", \"Nasdaq\", \"XLK\", \"Apple\", \"Microsoft\", \"Google\", \"Amazon\",\n",
    "    \"tech\", \"macro\", \"Fed\", \"CPI\", \"Treasury\"\n",
    "]\n",
    "\n",
    "all_articles = []\n",
    "\n",
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    date_str = current_date.strftime('%Y-%m-%d')\n",
    "    print(f\"Fetching news for {date_str}...\")\n",
    "\n",
    "    try:\n",
    "        # Finnhub: prendi gli ultimi articoli \"company\" (puoi anche usare \"general\")\n",
    "        articles = finnhub_client.general_news(category='company', min_id=0)\n",
    "\n",
    "        for article in articles:\n",
    "            # Converti timestamp a data YYYY-MM-DD\n",
    "            article_date = datetime.utcfromtimestamp(article['datetime']).strftime('%Y-%m-%d')\n",
    "\n",
    "            # Filtra solo articoli del giorno corrente\n",
    "            if article_date != date_str:\n",
    "                continue\n",
    "\n",
    "            title = article.get('headline', '').lower()\n",
    "            summary = article.get('summary', '').lower()\n",
    "            related = article.get('related', '').lower() if article.get('related') else \"\"\n",
    "\n",
    "            # Filtra solo articoli contenenti keyword rilevanti\n",
    "            if not any(k.lower() in title or k.lower() in summary or k.lower() in related for k in query_keywords):\n",
    "                continue\n",
    "\n",
    "            # Combina titolo e summary per FinBERT\n",
    "            text = f\"{article.get('headline', '')} {article.get('summary', '')}\"\n",
    "\n",
    "            # Applica FinBERT\n",
    "            sentiment = apply_finbert_sentiment(text, model, tokenizer)\n",
    "\n",
    "            all_articles.append({\n",
    "                'date': article_date,\n",
    "                'title': article.get('headline'),\n",
    "                'description': article.get('summary'),\n",
    "                'source': article.get('source'),\n",
    "                'url': article.get('url'),\n",
    "                **sentiment\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching news for {date_str}: {e}\")\n",
    "\n",
    "    # Pausa breve per rispettare eventuali rate limit\n",
    "    time.sleep(1)\n",
    "    current_date += timedelta(days=1)\n",
    "\n",
    "# Salva in DataFrame\n",
    "news_df = pd.DataFrame(all_articles)\n",
    "print(f\"\\nTotal articles fetched: {len(news_df)}\")\n",
    "\n",
    "# Salva in locale su CSV\n",
    "csv_path = \"finnhub_news_30days.csv\"\n",
    "news_df.to_csv(csv_path, index=False)\n",
    "print(f\"✅ News saved to {csv_path}\")\n",
    "\n",
    "# Mostra le prime righe\n",
    "news_df.head()"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching news for 2025-12-01...\n",
      "Fetching news for 2025-12-02...\n",
      "Fetching news for 2025-12-03...\n",
      "Fetching news for 2025-12-04...\n",
      "Fetching news for 2025-12-05...\n",
      "Fetching news for 2025-12-06...\n",
      "Fetching news for 2025-12-07...\n",
      "Fetching news for 2025-12-08...\n",
      "Fetching news for 2025-12-09...\n",
      "Fetching news for 2025-12-10...\n",
      "Fetching news for 2025-12-11...\n",
      "Fetching news for 2025-12-12...\n",
      "Fetching news for 2025-12-13...\n",
      "Fetching news for 2025-12-14...\n",
      "Fetching news for 2025-12-15...\n",
      "Fetching news for 2025-12-16...\n",
      "Fetching news for 2025-12-17...\n",
      "Fetching news for 2025-12-18...\n",
      "Fetching news for 2025-12-19...\n",
      "Fetching news for 2025-12-20...\n",
      "Fetching news for 2025-12-21...\n",
      "Fetching news for 2025-12-22...\n",
      "Fetching news for 2025-12-23...\n",
      "Fetching news for 2025-12-24...\n",
      "Fetching news for 2025-12-25...\n",
      "Fetching news for 2025-12-26...\n",
      "Fetching news for 2025-12-27...\n",
      "Fetching news for 2025-12-28...\n",
      "Fetching news for 2025-12-29...\n",
      "Fetching news for 2025-12-30...\n",
      "\n",
      "Total articles fetched: 12\n",
      "✅ News saved to finnhub_news_30days.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "         date                                              title  \\\n",
       "0  2025-12-29  Trump eyes January for announcement of Powell ...   \n",
       "1  2025-12-29  The Fed has gone into hibernation. Tuesday may...   \n",
       "2  2025-12-29  These biotech stocks are getting hammered by s...   \n",
       "3  2025-12-30  Fed minutes show deep split over interest-rate...   \n",
       "4  2025-12-30  Buying bitcoin is no longer a thing for this D...   \n",
       "\n",
       "                                         description       source  \\\n",
       "0  President Donald Trump on Monday revived his t...  MarketWatch   \n",
       "1  Federal Reserve officials are clearly divided ...  MarketWatch   \n",
       "2  Shares of both Ultragenyx and Mereo BioPharma ...  MarketWatch   \n",
       "3  The committee voted 9-3 to lower the benchmark...    Bloomberg   \n",
       "4  Prenetics has ditched its bitcoin-buying strat...  MarketWatch   \n",
       "\n",
       "                                                 url  negative   neutral  \\\n",
       "0  https://www.marketwatch.com/story/trump-eyes-j...  0.019851  0.775838   \n",
       "1  https://www.marketwatch.com/story/the-fed-has-...  0.012653  0.909969   \n",
       "2  https://www.marketwatch.com/story/these-biotec...  0.010712  0.971915   \n",
       "3  https://www.bloomberg.com/news/articles/2025-1...  0.027123  0.895052   \n",
       "4  https://www.marketwatch.com/story/buying-bitco...  0.020182  0.782715   \n",
       "\n",
       "   positive  compound  \n",
       "0  0.204311  0.184460  \n",
       "1  0.077378  0.064725  \n",
       "2  0.017373  0.006660  \n",
       "3  0.077825  0.050702  \n",
       "4  0.197103  0.176921  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>negative</th>\n",
       "      <th>neutral</th>\n",
       "      <th>positive</th>\n",
       "      <th>compound</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-12-29</td>\n",
       "      <td>Trump eyes January for announcement of Powell ...</td>\n",
       "      <td>President Donald Trump on Monday revived his t...</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>https://www.marketwatch.com/story/trump-eyes-j...</td>\n",
       "      <td>0.019851</td>\n",
       "      <td>0.775838</td>\n",
       "      <td>0.204311</td>\n",
       "      <td>0.184460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-12-29</td>\n",
       "      <td>The Fed has gone into hibernation. Tuesday may...</td>\n",
       "      <td>Federal Reserve officials are clearly divided ...</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>https://www.marketwatch.com/story/the-fed-has-...</td>\n",
       "      <td>0.012653</td>\n",
       "      <td>0.909969</td>\n",
       "      <td>0.077378</td>\n",
       "      <td>0.064725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-12-29</td>\n",
       "      <td>These biotech stocks are getting hammered by s...</td>\n",
       "      <td>Shares of both Ultragenyx and Mereo BioPharma ...</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>https://www.marketwatch.com/story/these-biotec...</td>\n",
       "      <td>0.010712</td>\n",
       "      <td>0.971915</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>0.006660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>Fed minutes show deep split over interest-rate...</td>\n",
       "      <td>The committee voted 9-3 to lower the benchmark...</td>\n",
       "      <td>Bloomberg</td>\n",
       "      <td>https://www.bloomberg.com/news/articles/2025-1...</td>\n",
       "      <td>0.027123</td>\n",
       "      <td>0.895052</td>\n",
       "      <td>0.077825</td>\n",
       "      <td>0.050702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-12-30</td>\n",
       "      <td>Buying bitcoin is no longer a thing for this D...</td>\n",
       "      <td>Prenetics has ditched its bitcoin-buying strat...</td>\n",
       "      <td>MarketWatch</td>\n",
       "      <td>https://www.marketwatch.com/story/buying-bitco...</td>\n",
       "      <td>0.020182</td>\n",
       "      <td>0.782715</td>\n",
       "      <td>0.197103</td>\n",
       "      <td>0.176921</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-01T17:59:53.254136Z",
     "start_time": "2026-01-01T17:59:50.134963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#CARICARE SU HOPSWORKS NON VA\n",
    "import os\n",
    "import hopsworks\n",
    "import pandas as pd\n",
    "\n",
    "# Assicurati che news_df abbia la colonna 'date' in formato datetime64[ms]\n",
    "news_df['date'] = pd.to_datetime(news_df['date']).astype('datetime64[ms]')\n",
    "\n",
    "# 1️⃣ Login\n",
    "project = hopsworks.login(\n",
    "    api_key_value=os.getenv(\"HOPSWORKS_API_KEY\"),\n",
    "    project=os.getenv(\"HOPSWORKS_PROJECT_NAME\")\n",
    ")\n",
    "fs = project.get_feature_store()\n",
    "\n",
    "fg_name = \"news_sentiment_raw\"\n",
    "fg_version = 1\n",
    "\n",
    "# 2️⃣ Controlla se il feature group esiste\n",
    "try:\n",
    "    news_fg = fs.get_feature_group(name=fg_name, version=fg_version)\n",
    "    if news_fg is None:\n",
    "        raise ValueError(\"get_feature_group returned None\")\n",
    "    print(f\"✓ Feature group '{fg_name}' esistente\")\n",
    "except Exception:\n",
    "    print(f\"✓ Feature group '{fg_name}' non trovato, lo creo...\")\n",
    "    news_fg = fs.create_feature_group(\n",
    "        name=fg_name,\n",
    "        version=fg_version,\n",
    "        description=\"Article-level news sentiment from Finnhub + FinBERT\",\n",
    "        primary_key=['date', 'url'],\n",
    "        online_enabled=False\n",
    "    )\n",
    "    print(f\"✓ Feature group '{fg_name}' creato\")\n",
    "\n",
    "# 3️⃣ Inserimento dati\n",
    "if news_fg is not None:\n",
    "    print(f\"✓ Inserisco {len(news_df)} articoli nel feature group...\")\n",
    "    job = news_fg.insert(news_df, write_options={\"wait_for_job\": True})\n",
    "    print(\"✅ News sentiment data uploaded to Hopsworks!\")\n",
    "else:\n",
    "    print(\"❌ Feature group non creato correttamente. Controlla i log del client Hopsworks.\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-01 18:59:50,138 INFO: Closing external client and cleaning up certificates.\n",
      "2026-01-01 18:59:50,140 INFO: Connection closed.\n",
      "2026-01-01 18:59:50,141 INFO: Initializing external client\n",
      "2026-01-01 18:59:50,141 INFO: Base URL: https://c.app.hopsworks.ai:443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "UserWarning: The installed hopsworks client version 4.6.0 may not be compatible with the connected Hopsworks backend version 4.2.2. \n",
      "To ensure compatibility please install the latest bug fix release matching the minor version of your backend (4.2) by running 'pip install hopsworks==4.2.*'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-01 18:59:51,333 INFO: Python Engine initialized.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/1272010\n",
      "✓ Feature group 'news_sentiment_raw' esistente\n",
      "✓ Inserisco 12 articoli nel feature group...\n"
     ]
    },
    {
     "ename": "FeatureStoreException",
     "evalue": "Failed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRestAPIError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hopsworks_common/core/variable_api.py:126\u001B[0m, in \u001B[0;36mVariableApi.get_loadbalancer_external_domain\u001B[0;34m(self, service)\u001B[0m\n\u001B[1;32m    125\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 126\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_variable\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mloadbalancer_external_domain_\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mservice\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m RestAPIError \u001B[38;5;28;01mas\u001B[39;00m err:\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hopsworks_common/core/variable_api.py:52\u001B[0m, in \u001B[0;36mVariableApi.get_variable\u001B[0;34m(self, variable)\u001B[0m\n\u001B[1;32m     51\u001B[0m path_params \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvariables\u001B[39m\u001B[38;5;124m\"\u001B[39m, variable]\n\u001B[0;32m---> 52\u001B[0m domain \u001B[38;5;241m=\u001B[39m \u001B[43m_client\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_send_request\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mGET\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpath_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m domain[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msuccessMessage\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hopsworks_common/decorators.py:48\u001B[0m, in \u001B[0;36mconnected.<locals>.if_connected\u001B[0;34m(inst, *args, **kwargs)\u001B[0m\n\u001B[1;32m     47\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m NoHopsworksConnectionError\n\u001B[0;32m---> 48\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43minst\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hopsworks_common/client/base.py:186\u001B[0m, in \u001B[0;36mClient._send_request\u001B[0;34m(self, method, path_params, query_params, headers, data, stream, files, with_base_path_params)\u001B[0m\n\u001B[1;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m response\u001B[38;5;241m.\u001B[39mstatus_code \u001B[38;5;241m/\u001B[39m\u001B[38;5;241m/\u001B[39m \u001B[38;5;241m100\u001B[39m \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m--> 186\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mRestAPIError(url, response)\n\u001B[1;32m    188\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m stream:\n",
      "\u001B[0;31mRestAPIError\u001B[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/variables/loadbalancer_external_domain_datanode). Server response: \nHTTP code: 404, HTTP reason: Not Found, body: b'{\"errorCode\":100050,\"usrMsg\":\"Variable: loadbalancer_external_domain_datanodenot found\",\"errorMsg\":\"Requested variable not found\"}', error code: 100050, error msg: Requested variable not found, user msg: Variable: loadbalancer_external_domain_datanodenot found",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mFeatureStoreException\u001B[0m                     Traceback (most recent call last)",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hsfs/core/delta_engine.py:285\u001B[0m, in \u001B[0;36mDeltaEngine._setup_delta_rs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    284\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 285\u001B[0m     datanode_ip \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variable_api\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_loadbalancer_external_domain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdatanode\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\n\u001B[1;32m    287\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    288\u001B[0m     _logger\u001B[38;5;241m.\u001B[39mdebug(\n\u001B[1;32m    289\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSetting HOPSFS_CLOUD_DATANODE_HOSTNAME_OVERRIDE to \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdatanode_ip\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    290\u001B[0m     )\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hopsworks_common/core/variable_api.py:129\u001B[0m, in \u001B[0;36mVariableApi.get_loadbalancer_external_domain\u001B[0;34m(self, service)\u001B[0m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m err\u001B[38;5;241m.\u001B[39mSTATUS_CODE_NOT_FOUND:\n\u001B[0;32m--> 129\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m FeatureStoreException(\n\u001B[1;32m    130\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mClient could not get \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mLOADBALANCER_SERVICES[service]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m service hostname from \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    131\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mloadbalancer_external_domain_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mservice\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    132\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe variable is either not set or empty in Hopsworks cluster configuration.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    133\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01merr\u001B[39;00m\n\u001B[1;32m    134\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[0;31mFeatureStoreException\u001B[0m: Client could not get datanode service hostname from loadbalancer_external_domain_datanode. The variable is either not set or empty in Hopsworks cluster configuration.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mFeatureStoreException\u001B[0m                     Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[20], line 38\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m news_fg \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✓ Inserisco \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(news_df)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m articoli nel feature group...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m     job \u001B[38;5;241m=\u001B[39m \u001B[43mnews_fg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnews_df\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwrite_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mwait_for_job\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     39\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m✅ News sentiment data uploaded to Hopsworks!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hsfs/feature_group.py:3454\u001B[0m, in \u001B[0;36mFeatureGroup.insert\u001B[0;34m(self, features, overwrite, operation, storage, write_options, validation_options, wait, transformation_context, transform)\u001B[0m\n\u001B[1;32m   3444\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mall\u001B[39m(\n\u001B[1;32m   3445\u001B[0m     [\n\u001B[1;32m   3446\u001B[0m         \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_id,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   3450\u001B[0m ):\n\u001B[1;32m   3451\u001B[0m     \u001B[38;5;66;03m# New delta FG allow for change data capture query\u001B[39;00m\n\u001B[1;32m   3452\u001B[0m     write_options[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta.enableChangeDataFeed\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m-> 3454\u001B[0m job, ge_report \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_feature_group_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minsert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3455\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3456\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeature_dataframe\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_dataframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3457\u001B[0m \u001B[43m    \u001B[49m\u001B[43moverwrite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverwrite\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3458\u001B[0m \u001B[43m    \u001B[49m\u001B[43moperation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moperation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3459\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3460\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwrite_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwrite_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3461\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43msave_report\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mvalidation_options\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3462\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransformation_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransformation_context\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3463\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtransform\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3464\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3466\u001B[0m \u001B[38;5;66;03m# Compute stats in client if there is no backfill job:\u001B[39;00m\n\u001B[1;32m   3467\u001B[0m \u001B[38;5;66;03m# - spark engine: always compute in client\u001B[39;00m\n\u001B[1;32m   3468\u001B[0m \u001B[38;5;66;03m# - python engine: only compute if FG is offline only (no backfill job)\u001B[39;00m\n\u001B[1;32m   3469\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m engine\u001B[38;5;241m.\u001B[39mget_type()\u001B[38;5;241m.\u001B[39mstartswith(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mspark\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstream:\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hsfs/core/feature_group_engine.py:247\u001B[0m, in \u001B[0;36mFeatureGroupEngine.insert\u001B[0;34m(self, feature_group, feature_dataframe, overwrite, operation, storage, write_options, validation_options, transformation_context, transform)\u001B[0m\n\u001B[1;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m overwrite:\n\u001B[1;32m    244\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_feature_group_api\u001B[38;5;241m.\u001B[39mdelete_content(feature_group)\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m (\n\u001B[0;32m--> 247\u001B[0m     \u001B[43mengine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_instance\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_dataframe\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    248\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_group\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    249\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_dataframe\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    250\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mbulk_insert\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moverwrite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43moperation\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfeature_group\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monline_enabled\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    252\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstorage\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    253\u001B[0m \u001B[43m        \u001B[49m\u001B[43moffline_write_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    254\u001B[0m \u001B[43m        \u001B[49m\u001B[43monline_write_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    255\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m,\n\u001B[1;32m    256\u001B[0m     ge_report,\n\u001B[1;32m    257\u001B[0m )\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hsfs/engine/python.py:1084\u001B[0m, in \u001B[0;36mEngine.save_dataframe\u001B[0;34m(self, feature_group, dataframe, operation, online_enabled, storage, offline_write_options, online_write_options, validation_id)\u001B[0m\n\u001B[1;32m   1082\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m engine\u001B[38;5;241m.\u001B[39mget_type() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpython\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m   1083\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m feature_group\u001B[38;5;241m.\u001B[39mtime_travel_format \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDELTA\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 1084\u001B[0m         delta_engine_instance \u001B[38;5;241m=\u001B[39m \u001B[43mdelta_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDeltaEngine\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1085\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeature_store_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_group\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_store_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1086\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeature_store_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_group\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeature_store_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1087\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfeature_group\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_group\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1088\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspark_context\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1089\u001B[0m \u001B[43m            \u001B[49m\u001B[43mspark_session\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1090\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1091\u001B[0m         delta_engine_instance\u001B[38;5;241m.\u001B[39msave_delta_fg(\n\u001B[1;32m   1092\u001B[0m             dataframe,\n\u001B[1;32m   1093\u001B[0m             write_options\u001B[38;5;241m=\u001B[39moffline_write_options,\n\u001B[1;32m   1094\u001B[0m             validation_id\u001B[38;5;241m=\u001B[39mvalidation_id,\n\u001B[1;32m   1095\u001B[0m         )\n\u001B[1;32m   1096\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1097\u001B[0m     \u001B[38;5;66;03m# for backwards compatibility\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hsfs/core/delta_engine.py:76\u001B[0m, in \u001B[0;36mDeltaEngine.__init__\u001B[0;34m(self, feature_store_id, feature_store_name, feature_group, spark_session, spark_context)\u001B[0m\n\u001B[1;32m     74\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_api \u001B[38;5;241m=\u001B[39m variable_api\u001B[38;5;241m.\u001B[39mVariableApi()\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_project_api \u001B[38;5;241m=\u001B[39m project_api\u001B[38;5;241m.\u001B[39mProjectApi()\n\u001B[0;32m---> 76\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_setup_delta_rs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/ProjectsScalable/.venv/lib/python3.10/site-packages/hsfs/core/delta_engine.py:293\u001B[0m, in \u001B[0;36mDeltaEngine._setup_delta_rs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    291\u001B[0m     os\u001B[38;5;241m.\u001B[39menviron[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mHOPSFS_CLOUD_DATANODE_HOSTNAME_OVERRIDE\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m datanode_ip\n\u001B[1;32m    292\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m FeatureStoreException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m FeatureStoreException(\n\u001B[1;32m    294\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    295\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    297\u001B[0m user_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_project_api\u001B[38;5;241m.\u001B[39mget_user_info()\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124musername\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    299\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m user_name:\n",
      "\u001B[0;31mFeatureStoreException\u001B[0m: Failed to write to delta table in external cluster. Make sure datanode load balancer has been setup on the cluster."
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
